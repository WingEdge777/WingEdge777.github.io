{"meta":{"title":"WingEdge777","subtitle":"","description":"WingEdge777's blog","author":"WingEdge777","url":"http://example.com","root":"/"},"pages":[{"title":"","date":"2025-10-31T09:48:42.980Z","updated":"2025-10-31T09:48:42.980Z","comments":true,"path":"4.4.html","permalink":"http://example.com/4.4.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"","date":"2025-10-31T09:48:42.980Z","updated":"2025-10-31T09:48:42.980Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"下面写关于自己的内容 关于我"},{"title":"categories","date":"2025-10-19T13:24:53.000Z","updated":"2025-10-31T09:48:42.980Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2025-10-19T13:25:28.000Z","updated":"2025-10-31T09:48:42.981Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"大语言模型 think budget","slug":"大语言模型 think budget","date":"2025-10-26T09:13:27.000Z","updated":"2025-10-31T09:48:42.980Z","comments":true,"path":"2025/10/26/大语言模型 think budget/","permalink":"http://example.com/2025/10/26/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20think%20budget/","excerpt":"从 OpenAI 开始提出推理模型开始，思考模型已经逐渐成为了学界和业界的热点，国内开源模型两巨头 qwen 和 deepseek，都有思考模式和非思考模式。尽管深度思考模型在推理任务上表现出色，但它们在推理过程中需要产生大量的思考信息，然后才输出最终结果。这导致了思考模型在推理任务上需要消耗大量的计算资源和时间。","text":"从 OpenAI 开始提出推理模型开始，思考模型已经逐渐成为了学界和业界的热点，国内开源模型两巨头 qwen 和 deepseek，都有思考模式和非思考模式。尽管深度思考模型在推理任务上表现出色，但它们在推理过程中需要产生大量的思考信息，然后才输出最终结果。这导致了思考模型在推理任务上需要消耗大量的计算资源和时间。 我不知道有多少人在使用思考模型，但笔者个人对思考模型的推理性能(性能吞吐)是极度不满意的，因此笔者一直避免使用思考模型，但人在江湖身不由己，有时候不得不使用思考模型。 本文尝试从限制 think token 长度的角度降低推理模型的推理时延。其实千问官网已经提供了带 think budget的 demo，阿里云平台提供的api接口也有thinkbudget功能，但在当前开源 serving 框架里却依然没有一个开箱即用的实现，因此笔者分享个人的实现方法。 首先说一下 think budget 的实现，其原理非常简单，就是在模型推理时，当输出的 token 长度 超过 think budget 时，就强行输出 think 停止 token id，从而达到限制思考长度的目的。qwen 的官方文档对此也有详细的说明。 更具体的, 笔者在此分别给出两种实现方法： 两种方法Custom Logits Processor第一种是从 server 端考虑，我们可以添加自定义的 logits processor，具体实现和 qwen 的官方文档所说类似，需要在具体的 serving 框架中实现，其实当前 sglang 也有待 merge 的 PR， 但不知为何一直没有被合并。该实现看似修改代码较多，但原理依然如前文所述并不复杂。笔者还找到网上有一个公开的 transformers库 使用的 logits processor 实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#| filename: thinking_budget_processor.py#| language: pythonfrom transformers.generate import LogitsProcessorclass ThinkingTokenBudgetProcessor(LogitsProcessor): &quot;&quot;&quot; A processor where after a maximum number of tokens are generated, a &lt;/think&gt; token is added at the end to stop the thinking generation, and then it will continue to generate the response. &quot;&quot;&quot; def __init__(self, tokenizer, max_thinking_tokens=None): self.tokenizer = tokenizer self.max_thinking_tokens = max_thinking_tokens self.think_end_token = self.tokenizer.encode(&quot;&lt;/think&gt;&quot;, add_special_tokens=False)[0] self.nl_token = self.tokenizer.encode(&quot;\\n&quot;, add_special_tokens=False)[0] self.tokens_generated = 0 self.stopped_thinking = False self.neg_inf = float(&#x27;-inf&#x27;) def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -&gt; torch.FloatTensor: self.tokens_generated += 1 if self.max_thinking_tokens == 0 and not self.stopped_thinking and self.tokens_generated &gt; 0: scores[:] = self.neg_inf scores[0][self.nl_token] = 0 scores[0][self.think_end_token] = 0 self.stopped_thinking = True return scores if self.max_thinking_tokens is not None and not self.stopped_thinking: if (self.tokens_generated / self.max_thinking_tokens) &gt; .95: scores[0][self.nl_token] = scores[0][self.think_end_token] * (1 + (self.tokens_generated / self.max_thinking_tokens)) scores[0][self.think_end_token] = ( scores[0][self.think_end_token] * (1 + (self.tokens_generated / self.max_thinking_tokens)) ) if self.tokens_generated &gt;= (self.max_thinking_tokens - 1): if self.tokens_generated == self.max_thinking_tokens-1: scores[:] = self.neg_inf scores[0][self.nl_token] = 0 else: scores[:] = self.neg_inf scores[0][self.think_end_token] = 0 self.stopped_thinking = True return scores 以上代码转载自: https://muellerzr.github.io/til/end_thinking.html 已有朱玉在前，笔者也无需多说了。 Double-Query Think Budget第二种笔者称之为 double-query，无需修改框架或自定义 logits processor, 通过两次调用的方式，来实现 think budget。 我们将对模型的调用封装为两次调用：第一次调用时，设置 max_tokens 为 think budget 大小，收到输出结果后，先判断已输出的内容中是否已经包含了 eos 或 think 停止 token id，如果包含，则直接返回；否则，我们将进行第二次调用时，设置 max_tokens 为剩余的 token 数量，之后收到的输出结果，就是模型最终要输出的内容了。 此处以调用 sglang qwen 模型 server 代码 为例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import asyncioimport aiohttpimport jsondef create_bench_client_session(): BENCH_AIOHTTP_TIMEOUT_SECONDS = 6 * 60 * 60 # 6 hours BENCH_AIOHTTP_READ_BUFSIZE_BYTES = 10 * 1024**2 # 10 MB aiohttp_timeout = aiohttp.ClientTimeout(total=BENCH_AIOHTTP_TIMEOUT_SECONDS) return aiohttp.ClientSession( timeout=aiohttp_timeout, read_bufsize=BENCH_AIOHTTP_READ_BUFSIZE_BYTES )api_url = &quot;http://10.60.68.98:30000/generate&quot;async def run(think_budget = 512, max_new_tokens=4096): query = &quot;选购手机该看哪些参数？详细介绍一下&quot; prompt = f&quot;&lt;|im_start|&gt;user\\n&#123;query&#125;&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n&lt;think&gt;\\n&quot; payload = &#123; &quot;text&quot;: prompt, &quot;sampling_params&quot;: &#123; &quot;top_k&quot;: 20, &quot;top_p&quot;: 0.8, &quot;max_new_tokens&quot;: think_budget, &quot;ignore_eos&quot;: False, &#125;, &quot;stream&quot;: False, &quot;logprob_start_len&quot;: -1, &#125; async with create_bench_client_session() as session: async with session.post( url=api_url, json=payload ) as res: data = await res.text() # print(data) res = json.loads(data) text = res[&quot;text&quot;] if res[&quot;meta_info&quot;][&quot;finish_reason&quot;][&quot;type&quot;] != &quot;stop&quot;: if &quot;&lt;/think&gt;&quot; in res[&quot;text&quot;]: payload[&quot;text&quot;] = prompt + res[&quot;text&quot;] else: payload[&quot;text&quot;] = prompt + res[&quot;text&quot;] + &quot;\\n\\nConsidering the limited time by the user, I have to give the solution based on the thinking directly now.\\n&lt;/think&gt;\\n\\n&quot; payload[&quot;sampling_params&quot;][&quot;max_new_tokens&quot;] = max_new_tokens - think_budget async with create_bench_client_session() as session: async with session.post( url=api_url, json=payload ) as res: data = await res.text() # print(data) text = text + &quot;\\n\\nConsidering the limited time by the user, I have to give the solution based on the thinking directly now.\\n&lt;/think&gt;\\n\\n&quot; + json.loads(data)[&quot;text&quot;] print(text) return textif __name__ == &quot;__main__&quot;: asyncio.run(run()) 单一 server 或结合 cache-aware 调度的 serve r都可以利用前缀 kv cache, 因为开销不大。如此可以简单有效地限制最大思考长度，保障 SLA。","categories":[],"tags":[{"name":"LLM serving","slug":"LLM-serving","permalink":"http://example.com/tags/LLM-serving/"},{"name":"sglang","slug":"sglang","permalink":"http://example.com/tags/sglang/"}]}],"categories":[],"tags":[{"name":"LLM serving","slug":"LLM-serving","permalink":"http://example.com/tags/LLM-serving/"},{"name":"sglang","slug":"sglang","permalink":"http://example.com/tags/sglang/"}]}